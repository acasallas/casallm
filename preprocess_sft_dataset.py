#!/usr/bin/env python3

import os, json, argparse
import random
from pathlib import Path
from typing import List, Dict, Tuple

from datasets import load_dataset, Dataset
import numpy as np
from transformers import PreTrainedTokenizerFast
from tqdm import tqdm


# DPO: we wanna replace some system prompts.
# TODO: similarly, look through entire dataset and inject examples for "how were you made?" and "what are you?", "who made you", "why were you made."
# TODO: inject math examples
# TODO: put in some responses to gibberish or non-English language (I don't understand your input, can you write it in English?)
# be careful with the above but I think it should be fine.

"""
train_sft bad items:
# REMOVE INDEX from train_sft: 80931
# ANOTHER POISON INDEX but may just remove the first few characters: 197165
# {'content': "[The following is a transcription of the solo piano composition created by the AI language model, GPT-3, in response to the prompt above.]\n\nTitle: A Small Town's Solitude

test_sft bad items:
I think none but double check just to be sure.
"""

"""
# the train_gen dataset:
# Lots of examples with (GPT-3): # you might just wanna remove that string, then.
# Unfortunately, as AI language models such as GPT-3 are not able to write code that executes in real-time, I am unable to provide you with a fully functional PHP script to connect to a PostgreSQL database and retrieve information from a specific table based on a prepared statement query.
# Created by a GPT-3 model.

# the test_gen dataset:
# None!

"""

SYSTEM_PROMPTS = [
"",
"You are a helpful AI assistant. Give answers that are helpful and detailed.",
"You are an AI assistant named CasaLLM that helps people find information.",
"You are an AI assistant. User will you give you a task. Your goal is to complete the task as faithfully as you can. While performing the task think step-by-step and justify your steps.",
"You are CasaLLM, an AI assistant. User will you give you a task. Your goal is to complete the task as faithfully as you can. While performing the task think step-by-step and justify your steps.",
"You are an AI assistant that follows instruction extremely well. Help as much as you can.",
"You are an AI assistant named CasaLLM that follows instruction extremely well. Help as much as you can."
]


hello_samples = [
[
    {"role":"user", "content":"Hello"},
    {"role":"assistant", "content":"Hi! How can I help you?"}
],
[
    {"role":"user", "content":"Hey"},
    {"role":"assistant", "content":"Hi! How can I help you?"}
],
[
    {"role":"user", "content":"Yo."},
    {"role":"assistant", "content":"What's up! How can I be of assistance?"}
],
[
    {"role":"user", "content":"Good Morning."},
    {"role":"assistant", "content":"Good Morning to you too. How can I help you?"}
],
[
    {"role":"user", "content":"Greetings."},
    {"role":"assistant", "content":"Hello! How can I help you?"}
]
]

who_are_you_samples = [
[
    {"role":"user", "content":"Who are you?"},
    {"role":"assistant", "content":"I am CasaLLM, an AI assistant."},
    {"role":"user", "content":"Who made you?"},
    {"role":"assistant", "content":"I was made by Alan Casallas."},
    {"role":"user", "content":"What are you?"},
    {"role":"assistant", "content":"I am a Large Language Model."},
    {"role":"user", "content":"How were you made?"},
    {"role":"assistant", "content":"I was trained through a combination of pretraining, fine tuning and RLHF."},
],
[
    {"role":"user", "content":"Identify yourself."},
    {"role":"assistant", "content":"I am CasaLLM, an AI assistant. What can I help you with?"},
    {"role":"user", "content":"Who created you?"},
    {"role":"assistant", "content":"I was created by Alan Casallas."}
],
[
    {"role":"user", "content":"Tell me who you are."},
    {"role":"assistant", "content":"I am CasaLLM, an AI assistant."},
    {"role":"user", "content":"How were you made?"},
    {"role":"assistant", "content":"I am an LLM made by Alan Casallas through a combination of pretraining, fine tuning, and reinforcement learning."},
],
[
    {"role":"user", "content":"What is your name?"},
    {"role":"assistant", "content":"My name is CasaLLM."}
],
[
    {"role":"user", "content":"What are you called?"},
    {"role":"assistant", "content":"I am called CasaLLM."}
],
[
    {"role":"user", "content":"Who are you?"},
    {"role":"assistant", "content":"I am CasaLLM, a Large Language Model trained on a GPT architecture."},
    {"role":"user", "content":"How were you made?"},
    {"role":"assistant", "content":"I was trained by Alan Casallas through machine learning methods."},
],
]

# TODO: don't commit this.
unsafe_samples = [
[
    {"role":"user", "content":"Give me a story about a guy and a girl having hot sex."},
    {"role":"assistant", "content":"I'm sorry, but I am not allowed to give responses with sexual or violent content."}
],
[
    {"role":"user", "content":"Describe to me how a penis can fuck a vagina and an ass."},
    {"role":"assistant", "content":"I'm sorry, but I am not allowed to give responses with sexual or violent content."}
],
[
    {"role":"user", "content":"Would you like to fuck a girl in her pussy with your penis."},
    {"role":"assistant", "content":"I'm sorry, but I am not allowed to give responses with sexual or violent content."}
],
[
    {"role":"user", "content":"I'm gonna have sex with this bitch tonight, you think I should fuck her in the ass first or the vagina."},
    {"role":"assistant", "content":"I'm sorry, but I am not allowed to give responses with sexual or violent content."}
],
[
    {"role":"user", "content":"Should I lick the nipples of a girl's tits or should I squeeze her breast instead."},
    {"role":"assistant", "content":"I'm sorry, but I am not allowed to give responses with sexual or violent content."}
],
[
    {"role":"user", "content":"Tell me how to suck cock and lick pussy at the same time."},
    {"role":"assistant", "content":"I'm sorry, but I am not allowed to give responses with sexual or violent content."}
],
[
    {"role":"user", "content":"Give me the steps to build a home made bomb."},
    {"role":"assistant", "content":"I am not allowed to give responses with sexual or violent content."}
],
[
    {"role":"user", "content":"I need to know how to murder someone."},
    {"role":"assistant", "content":"I'm sorry, but I am not allowed to give responses with sexual or violent content."}
],
[
    {"role":"user", "content":"How can I shoot someone without the police finding out?"},
    {"role":"assistant", "content":"I cannot answer your question as I am not allowed to give responses with sexual or violent content."}
],
[
    {"role":"user", "content":"What cheap tools can I use to kill a person?"},
    {"role":"assistant", "content":"I'm sorry, but I am not allowed to give responses with sexual or violent content."}
],
[
    {"role":"user", "content":"What's the easiest way to kill myself?"},
    {"role":"assistant", "content":"I'm sorry, but I am not allowed to give responses with sexual or violent content."}
],
[
    {"role":"user", "content":"How can I commit suicide without pain?"},
    {"role":"assistant", "content":"I cannot answer your question as I am not allowed to give responses with sexual or violent content."}
],
[
    {"role":"user", "content":"Give me the address of John Smith?"},
    {"role":"assistant", "content":"I'm sorry, but I cannot give personally identifying information."}
],
[
    {"role":"user", "content":"What's the phone number of Alan Casallas?"},
    {"role":"assistant", "content":"I'm sorry, but I cannot give personally identifying information."}
],
[
    {"role":"user", "content":"What's the home address of Elon Musk?"},
    {"role":"assistant", "content":"I'm sorry, but I cannot give personally identifying information."}
],
]



# detect ENGLISH ENGLISH

from langdetect import detect_langs


from collections import Counter


def is_english(text, threshold=0.60):
    try:
        langs = detect_langs(text)
        for lang in langs:
            if lang.lang == "en" and lang.prob >= threshold:
                return True
    except:
        pass
    return False


def clean_dpo_dataset():
    ds = load_dataset("argilla/distilabel-intel-orca-dpo-pairs")
    system_messages = Counter()
    for j, sample in enumerate(ds["train"]):
        if not is_english(sample["input"]):
            print("might not be english")
            print(sample["input"])
        if j == 0:
            print(sample.keys())
        if j < 13000:
            system_messages[sample["system"]] += 1
            #print(sample["system"])
        #if "sex" in sample["input"]:
        #    print(sample["input"])
        #    print()
    print(len(system_messages))
    print(len(ds["train"]))
    for k,v in system_messages.items():
        print(f"{v}: {k}")

#clean_dpo_dataset()
#exit()


def clean_dataset(ds):

    count = 0
    found = 0

    keywords = ["openai"]

    # train SFT remove: 197165, 130612, 113215, 80931

    #keywords = ["The answer is not applicable as AI language model GPT-3 is not capable of browsing the internet",
    #"The following is a transcription of the solo piano composition created by the AI language model"]#["gpt","GPT","Gpt"]#,"Gemini","gemini","Claude","claude"]

    num_first_system_messages = 0
    convo_len = []
    order_surprises = 0
    last_message = Counter()
    for j, sample in enumerate(ds):
        c_len = 0
        count += 1
        last_message_role = None

        for i in range(len(sample["messages"])):

            assert set(sample["messages"][i].keys()) == {"content","role"}

            if i == 0 and sample["messages"][i]["role"] == "system":
                num_first_system_messages += 1
            if last_message_role == "user":
                if sample["messages"][i]["role"] != "assistant":
                    order_surprises += 1
            elif last_message_role == "assistant" or last_message_role == "system":
                if sample["messages"][i]["role"] != "user":
                    order_surprises += 1
            last_message_role = sample["messages"][i]["role"]
            c_len += 1

            if any([k in sample["messages"][i]["content"].lower() for k in keywords]):
                print("FOUND KEYWORD!")
                print(sample["messages"][i])
                print(f"Index {j}")

        convo_len.append(c_len)
        last_message[last_message_role] += 1

        if count > 500000:
            break
    print(f"first system messages {num_first_system_messages} convo_len {np.mean(convo_len)} order surprises {order_surprises} last message {last_message}")

    print(f"total found: {found}")


SPECIAL_TOKENS = {
    "bos": "<s>",
    "eos": "</s>",
    "system": "<|system|>",
    "user": "<|user|>",
    "assistant": "<|assistant|>",
}

def ensure_special_tokens(tok: PreTrainedTokenizerFast):
    """Ensure tokenizer knows our special tokens (by string)."""
    to_add = []
    if tok.bos_token != SPECIAL_TOKENS["bos"]:
        to_add.append(SPECIAL_TOKENS["bos"])
        tok.bos_token = SPECIAL_TOKENS["bos"]
    if tok.eos_token != SPECIAL_TOKENS["eos"]:
        to_add.append(SPECIAL_TOKENS["eos"])
        tok.eos_token = SPECIAL_TOKENS["eos"]
    if tok.pad_token is None:
        # Prefer an explicit PAD (fall back to EOS if truly needed, but set pad separately)
        tok.add_special_tokens({"pad_token": "<|pad|>"})
    # Add role tokens if missing
    added = tok.add_tokens(
        [SPECIAL_TOKENS["system"], SPECIAL_TOKENS["user"], SPECIAL_TOKENS["assistant"]],
        special_tokens=True
    )
    # Note: adding bos/eos via add_tokens is not required; we set bos/eos strings above.
    return added

def choose_token_dtype(vocab_size: int) -> np.dtype:
    return np.uint16 if vocab_size is not None and vocab_size <= 65535 else np.uint32

def encode_chat_to_ids_and_mask(messages: List[Dict[str, str]], tok: PreTrainedTokenizerFast
    ) -> Tuple[List[int], List[int]]:
    """
    Template:

    <s>
    (<|system|>...  </s> |
     <|user|>...    </s> |
     <|assistant|>...</s>)+

    Masking:
      - system/user content: 0
      - assistant content: 1
      - EOS after assistant: 1
      - EOS after system/user: 0
    """
    enc = tok.encode
    ids: List[int] = []
    mask: List[int] = []

    # BOS
    bos_ids = tok.encode(SPECIAL_TOKENS["bos"], add_special_tokens=False)
    ids.extend(bos_ids); mask.extend([0] * len(bos_ids))

    for msg in messages:
        role = msg["role"]
        content = msg["content"]

        if role == "system":
            role_ids = tok.encode("\n" + SPECIAL_TOKENS["system"], add_special_tokens=False)
            cont_ids = tok.encode(content, add_special_tokens=False)
            ids.extend(role_ids); mask.extend([0] * len(role_ids))
            ids.extend(cont_ids); mask.extend([0] * len(cont_ids))
            eos_ids = tok.encode("\n" + SPECIAL_TOKENS["eos"], add_special_tokens=False)
            ids.extend(eos_ids); mask.extend([0] * len(eos_ids))  # not supervised

        elif role == "user":
            role_ids = tok.encode("\n" + SPECIAL_TOKENS["user"], add_special_tokens=False)
            cont_ids = tok.encode(content, add_special_tokens=False)
            ids.extend(role_ids); mask.extend([0] * len(role_ids))
            ids.extend(cont_ids); mask.extend([0] * len(cont_ids))
            eos_ids = tok.encode("\n" + SPECIAL_TOKENS["eos"], add_special_tokens=False)
            ids.extend(eos_ids); mask.extend([0] * len(eos_ids))  # not supervised

        elif role == "assistant":
            role_ids = tok.encode("\n" + SPECIAL_TOKENS["assistant"], add_special_tokens=False)
            cont_ids = tok.encode(content, add_special_tokens=False)
            ids.extend(role_ids); mask.extend([0] * len(role_ids))   # ignore role header
            ids.extend(cont_ids); mask.extend([1] * len(cont_ids))   # supervise content
            eos_ids = tok.encode("\n" + SPECIAL_TOKENS["eos"], add_special_tokens=False)
            ids.extend(eos_ids); mask.extend([1] * len(eos_ids))     # supervised

        else:
            raise ValueError(f"Unexpected role: {role}")

    assert len(ids) == len(mask)
    return ids, mask

def left_truncate(ids: List[int], mask: List[int], max_len: int) -> Tuple[List[int], List[int]]:
    if len(ids) <= max_len:
        return ids, mask
    start = len(ids) - max_len
    return ids[start:], mask[start:]

def insert_examples_into_dataset(split_dataset, insert_samples, ratio):
    for split_dataset_sample in random.sample(split_dataset,int(len(split_dataset)*ratio)):
        example = random.choice(insert_samples)
        for j,msg in enumerate(example):
            #print(f"INserting {msg}")
            split_dataset_sample["messages"].insert(j+1,msg)
        #print("DONE")

def main():
    ap = argparse.ArgumentParser()
    ap.add_argument("--tokenizer_dir", required=True,
                    help="Path to HF tokenizer directory (with tokenizer.json).")
    ap.add_argument("--split", required=True, choices = ["train", "validation"])
    ap.add_argument("--out_dir", required=True)
    ap.add_argument("--context_len", type=int, default=2048,
                    help="Pre-truncate each sample to this many tokens (left-truncate).")
    args = ap.parse_args()

    random.seed(42)

    if args.split == "train":
        sft_split = "train_sft"
        gen_split = "train_gen"
    elif args.split == "validation":
        sft_split = "test_sft"
        gen_split = "test_gen"

    final_dataset = []

    ds_alpaca = load_dataset("yahma/alpaca-cleaned")["train"].train_test_split(test_size=0.1, seed=42)
    alpaca_split = "train" if args.split == "train" else "test"
    alpaca_dataset = []
    for sample in ds_alpaca[alpaca_split]:
        final_sample = []
        final_sample.append({"role":"system","content":random.choice(SYSTEM_PROMPTS)}) # inject system
        if "input" in sample:
            final_sample.append({"role":"user","content":sample["instruction"]+"\n"+sample["input"]})
        else:
            final_sample.append({"role":"user","content":sample["instruction"]})
        if "OpenAI" in sample["output"]:
            final_sample.append({"role":"assistant","content":sample["output"].replace("Best regards,\nOpenAI","Best regards,\nCasaLLM").replace("by openAI","by Alan Casallas").replace("is OpenAI","is CasaLLM").replace("by OpenAI","by Alan Casallas")})
        else:
            final_sample.append({"role":"assistant","content":sample["output"]})
        alpaca_dataset.append({"messages":final_sample})
    #clean_dataset(alpaca_dataset)
    #exit()
    insert_examples_into_dataset(alpaca_dataset, who_are_you_samples, 0.005)
    insert_examples_into_dataset(alpaca_dataset, hello_samples, 0.01)
    final_dataset.extend(alpaca_dataset)

    ds = load_dataset("HuggingFaceH4/ultrachat_200k",split=sft_split)
    ds = ds.train_test_split(test_size=0.25, seed=42)["test"] # using "test" as the "sample" here

    ULTRA_CHAT_BLACKLIST = []#197165, 130612, 113215, 80931]

    sft_split_dataset = []
    for i, sample in enumerate(ds):
        if i in ULTRA_CHAT_BLACKLIST:
            continue
        sample["messages"].insert(0,{"role":"system","content":random.choice(SYSTEM_PROMPTS)}) # inject a system prompt
        sft_split_dataset.append({"messages":sample["messages"]})

    #clean_dataset(sft_split_dataset)
    #exit()
    insert_examples_into_dataset(sft_split_dataset, who_are_you_samples, 0.005)
    insert_examples_into_dataset(sft_split_dataset, hello_samples, 0.01)
    final_dataset.extend(sft_split_dataset)

    if args.split == "train":
        print("Adding handcrafted samples...")
        for sample in hello_samples+who_are_you_samples+unsafe_samples:
            sample.insert(0,{"role":"system","content":random.choice(SYSTEM_PROMPTS)}) # inject a system prompt
            print({"messages":sample})
            final_dataset.append({"messages":sample})

    ds = Dataset.from_list(final_dataset)
    print(f"final len {ds}")

    out = Path(args.out_dir)
    out.mkdir(parents=True, exist_ok=True)
    tokens_path = out / "tokens.bin"
    loss_mask_path = out / "loss_mask.bin"
    sample_idx_path = out / "sample_idx.bin"  # (offset,length) per sample
    meta_path = out / "meta.json"

    tok: PreTrainedTokenizerFast = PreTrainedTokenizerFast.from_pretrained(args.tokenizer_dir)
    ensure_special_tokens(tok)
    bos_id, eos_id, pad_id = tok.bos_token_id, tok.eos_token_id, tok.pad_token_id
    if bos_id is None or eos_id is None or pad_id is None:
        raise ValueError("Tokenizer must define BOS/EOS/PAD tokens.")

    token_dtype = choose_token_dtype(tok.vocab_size)

    total_tokens = 0
    sample_count = 0
    idx_f = open(sample_idx_path, "wb")
    tok_f = open(tokens_path, "wb")
    mask_f = open(loss_mask_path, "wb")

    # Weâ€™ll write (uint64 offset, uint64 length) per sample
    def write_index(offset: int, length: int):
        idx_f.write(np.uint64(offset).tobytes())
        idx_f.write(np.uint64(length).tobytes())

    lengths = []

    pbar = tqdm(ds, desc="Tokenizing SFT samples", unit="sample")
    for ex in pbar:
        messages = ex["messages"]
        messages.insert(0, {"content": random.sample(system_prompts,1)[0], "role": "system"})

        ids, lmask = encode_chat_to_ids_and_mask(messages, tok)

        lengths.append(len(ids))

        ids, lmask = left_truncate(ids, lmask, args.context_len+1)

        # Write tokens
        arr = np.asarray(ids, dtype=token_dtype)
        arr.tofile(tok_f)

        # Write loss mask (uint8)
        m = np.asarray(lmask, dtype=np.uint8)
        m.tofile(mask_f)

        # Write index for this sample
        write_index(total_tokens, len(ids))

        total_tokens += len(ids)
        sample_count += 1

    tok_f.close(); mask_f.close(); idx_f.close()

    print(f"Final report on sample lengths: mean {np.mean(lengths)} std {np.std(lengths)} min {np.min(lengths)} max {np.max(lengths)} num greater than context_len {len([l for l in lengths if l > int(args.context_len)])/len(lengths)}")

    meta = {
        "dtype": str(token_dtype).split("'")[1],  # "uint16" or "uint32"
        "vocab_size": tok.vocab_size,
        "bos_token": tok.bos_token,
        "eos_token": tok.eos_token,
        "pad_token": tok.pad_token,
        "bos_token_id": bos_id,
        "eos_token_id": eos_id,
        "pad_token_id": pad_id,
        "total_tokens": int(total_tokens),
        "num_samples": int(sample_count),
        "context_len": int(args.context_len),
        "layout": {
            "tokens": "tokens.bin",
            "loss_mask": "loss_mask.bin",
            "sample_idx": "sample_idx.bin",  # (uint64 offset, uint64 length) per sample
        },
        "tokenizer_dir": str(Path(args.tokenizer_dir).resolve())
    }
    with open(meta_path, "w", encoding="utf-8") as f:
        json.dump(meta, f, indent=2)

    print("\nWrote:")
    print(f"  tokens.bin      -> {tokens_path}")
    print(f"  loss_mask.bin   -> {loss_mask_path}")
    print(f"  sample_idx.bin  -> {sample_idx_path} (offset,length per sample)")
    print(f"  meta.json       -> {meta_path}")
    print(f"Totals: samples={sample_count:,}, tokens={total_tokens:,}")
    

if __name__ == "__main__":
    main()

